# Counter-Based Sampling

This directory contains the counter-based sampling algorithm for PCAP file preprocessing.

## Overview

The counter-based sampling algorithm intelligently reduces the size of large PCAP files while maintaining the statistical properties and diversity of the original dataset. This is essential for creating manageable datasets for machine learning training without losing important traffic patterns.

## How It Works

### Algorithm Steps

1. **Packet Reading**: Load packets from the input PCAP file using Scapy
2. **Key Extraction**: Extract a 3-tuple key (source IP, destination IP, protocol) from each packet
3. **Pattern Counting**: Group packets by their keys and count occurrences of each pattern
4. **Balanced Sampling**:
   - Sample packets from each group proportionally to the desired ratio
   - Ensure minimum representation per traffic pattern (configurable via `min_per_key`)
5. **Gap Filling**: If total sampled packets < target, randomly select additional packets
6. **Output**: Write the filtered packets to a new PCAP file

### Key Features

- **Flow-aware**: Maintains the relationship between packets in the same flow
- **Balanced**: Ensures all traffic patterns are represented
- **Configurable**: Multiple sampling ratios and minimum guarantees
- **Batch Processing**: Process entire folders of PCAP files automatically

## Usage

### Single File Processing

```python
from Counter-Based Sampling import reduce_pcap_balanced

# Reduce a PCAP file to 10% of its original size
reduce_pcap_balanced(
    input_path="large_dataset.pcap",
    output_path="sampled_dataset.pcap",
    keep_ratio=0.1,  # Keep 10%
    min_per_key=100  # Minimum 100 packets per traffic pattern
)
```

### Batch Processing

```python
from Counter-Based Sampling import process_folder

# Process all PCAP files in a directory
# Creates multiple versions with different sampling ratios (5%, 10%, 15%, ..., 95%)
process_folder("./pcap_files/")
```

### Customizing the Script

Edit the main script to change the folder path:

```python
if __name__ == "__main__":
    folder_path = "./your-pcap-folder"  # Change this path
    process_folder(folder_path)
```

## Parameters

### `reduce_pcap_balanced()`

- **`input_path`** (str): Path to the input PCAP file
- **`output_path`** (str): Path for the output PCAP file
- **`keep_ratio`** (float): Fraction of packets to keep (0.0 to 1.0)
- **`min_per_key`** (int): Minimum packets to keep per traffic pattern (default: 100)

### `process_folder()`

- **`folder_path`** (str): Directory containing PCAP files to process

## Output

The algorithm generates multiple sampled versions of each PCAP file:

- `original_5.pcap` (5% of original)
- `original_10.pcap` (10% of original)
- `original_15.pcap` (15% of original)
- ...
- `original_95.pcap` (95% of original)

## Performance Considerations

- **Memory Usage**: Large PCAP files are loaded entirely into memory
- **Processing Time**: Depends on file size and number of unique traffic patterns
- **Disk Space**: Multiple output files are generated, requiring adequate storage

## Requirements

- Python 3.6+
- Scapy library for packet manipulation
- Collections module (built-in)

## Example Output

```
Processing file: traffic_capture.pcap
  → Generating traffic_capture_5.pcap with 5% of packets
  → Generating traffic_capture_10.pcap with 10% of packets
  → Generating traffic_capture_15.pcap with 15% of packets
  ...
```

## Use Cases

1. **Dataset Preparation**: Create smaller, manageable versions of large network captures
2. **Machine Learning**: Generate balanced training sets for DDoS detection models
3. **Performance Testing**: Test algorithms with different dataset sizes
4. **Storage Optimization**: Reduce storage requirements while preserving data diversity

## Integration with CNN Detection

The sampled PCAP files generated by this component can be directly used as input for the CNN DDoS detection component:

1. Run counter-based sampling on your raw PCAP files
2. Use the generated sampled files as input to `lucid_dataset_parser.py`
3. Train the CNN model on the processed, balanced dataset
